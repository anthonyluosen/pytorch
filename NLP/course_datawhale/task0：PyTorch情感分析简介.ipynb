{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task0：自然语言处理之PyTorch情感分析简介\n",
    "\n",
    "> 注：本期组队学习仅适用于需要PyTorch 1.8或以上版本的torchtext 0.9或以上版本。如果你使用的是torchtext 0.8，请点击[这里](https://github.com/bentrevett/pytorch-sentiment-analysis/tree/torchtext08)。\n",
    "\n",
    "本期学习使用的软件和版本为：[Pytorch1.8](https://github.com/pytorch/pytorch)，[torchtext0.9](https://github.com/pytorch/text) 和Python3.7\n",
    "\n",
    "本期组队学习主要从一下几个方面进行学习：\n",
    "\n",
    "- 利用RNN进行情感二分类\n",
    "- 利用RNN的各种变体，如LSTM, BiLSTM等进行情感二分类\n",
    "- 利用更快的模型FastText进行情感二分类\n",
    "- 利用CNN进行情感二分类\n",
    "- 情感多分类\n",
    "- 利用BERT进行情感分类\n",
    "    \n",
    "前两个Text将介绍情感分析的常用方法：递归神经网络（RNN）；第三个Text介绍了[FastText](https://arxiv.org/abs/1607.01759)模型；最后一个task的学习覆盖一个[卷积神经网络](https://arxiv.org/abs/1408.5882)（CNN）模型。\n",
    "\n",
    "还有两个额外的“附录”。第一部分介绍如何使用torchtext加载自己的数据集，第二部分简要介绍torchtext提供的经过预训练的单词嵌入。这部分自由学习，在组队学习中不做要求。\n",
    "\n",
    "## 环境配置\n",
    "\n",
    "①要安装Pytorch，请参阅[Pytorch网站](https://pytorch.org/get-started/locally)上的安装说明。\n",
    "\n",
    "②要安装torchtext，请执行以下操作：\n",
    "\n",
    "```bas\n",
    "pip install torchtext\n",
    "```\n",
    "\n",
    "③若安装速度较慢，可改为以下命令：\n",
    "\n",
    "```ba\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torchtext\n",
    "```\n",
    "\n",
    "④此外，我们还将使用spaCy来标记数据。要安装spaCy，可以按照[spaCy官网](https://spacy.io/usage)的指令来安装，或者执行以下命令：\n",
    "\n",
    "```猛击\n",
    "python -m venv .env\n",
    ".env\\Scripts\\activate\n",
    "pip install -U pip setuptools wheel\n",
    "pip install -U spacy[transformers,lookups]\n",
    "python -m spacy download zh_core_web_sm\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "⑤对于Taxt6，我们将使用transformers库，可以通过以下方式安装（更改为清华源）：\n",
    "\n",
    "```猛击\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple transformers\n",
    "```\n",
    "\n",
    "这些教程是使用的transformers版本为4.3。\n",
    "\n",
    "## 组队学习基本内容\n",
    "\n",
    "* task1- [情感分析baseline](https://github.com/datawhalechina/team-learning-nlp/blob/master/Emotional_Analysis/task1%20%EF%BC%9A%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90baseline.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "这一章主要介绍PyTorch with torchtext项目的工作流。我们将学习如何：加载数据、创建训练/测试/验证拆分、构建词汇表、创建数据迭代器、定义模型以及实现训练/评估/测试循环。该模型将简单但是性能较差，可以将其看作一个Baseline，可以用于学习整个情感分析的处理过程，在后续教程中我们将对此模型进行改进。\n",
    "\n",
    "* task2-[Updated情感分析 ](https://github.com/datawhalechina/team-learning-nlp/blob/master/Emotional_Analysis/task2%EF%BC%9AUpdated%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%20.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "现在我们已经学习了情感分析的基本工作流程，下面我们将学习如何改进模型：使用压缩填充序列、加载和使用预先训练word embedding、不同的优化器、不同的RNN体系结构、双向RNN、多层（又称深层）RNN和正则化。\n",
    "\n",
    "* task3-[Faster情感分析](https://github.com/datawhalechina/team-learning-nlp/blob/master/Emotional_Analysis/task3%EF%BC%9AFaster%20%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "在我们介绍了使用RNN的升级版本的情感分析之后，我们将研究一种不使用RNN的不同方法：我们将实现论文 [《Bag of Tricks for Efficient Text Classification》](https://arxiv.org/abs/1607.01759)中的模型，该论文已经放在了教程中，感兴趣的小伙伴可以参考一下。这个简单的模型实现了与第二节中的升级的情感分析相当的性能，但训练速度要快得多。\n",
    "\n",
    "* task4-[卷积情感分析](https://github.com/datawhalechina/team-learning-nlp/blob/master/Emotional_Analysis/task4%EF%BC%9A%E5%8D%B7%E7%A7%AF%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%20.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "接下来，我们将介绍用于情绪分析的卷积神经网络（CNN）。该模型将是[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)的实现。\n",
    "\n",
    "* task5-[多模型融合情绪分析](https://github.com/datawhalechina/team-learning-nlp/blob/master/Emotional_Analysis/task5%EF%BC%9A%E5%A4%9A%E7%B1%BB%E5%88%AB%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/5%20-%20Multi-class%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "这一章，我们将使用包含以上两种模型的处理形式，这在NLP中很常见。我们将使用Text4中的CNN模型和一个包含6个分类的新数据集。\n",
    "\n",
    "* task6-[使用Transformers进行情感分析](https://github.com/datawhalechina/team-learning-nlp/blob/master/Emotional_Analysis/task6：Transformers情感分析.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "这一章，我们将学习如何使用transformers库加载预训练的transformer模型，实现论文[BERT：Pre-training of Deep Bidirectional Transfoemers for Language Understanding](https://arxiv.org/abs/1810.04805)中的BERT模型（该论文也以放入教程中），并使用它完成文本的embeddings。这些embeddings可以输入到任何模型中来预测情绪，在这里，我们使用了一个门循环单元（GRU）。\n",
    "\n",
    "## 拓展（待更新……）\n",
    "\n",
    "* A-[在自己的数据集上使用torchtext]() [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/A%20-%20Using%20TorchText%20with%20Your%20Own%20Datasets.ipynb)\n",
    "\n",
    "因为本教程使用的数据集为TorchText的内置数据集，附录A说明了如何使用TorchText加载自己的数据集。\n",
    "\n",
    "* B-[再看word embedding]() [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/B%20-%20A%20Closer%20Look%20at%20Word%20Embeddings.ipynb)\n",
    "\n",
    "通过使用TorchText提供的预训练word embedding来查看类似单词，以及实现一个简单的，完全基于word embedding的拼写错误校正器。\n",
    "\n",
    "* C-[加载、保存和固定Word embedding]() [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/C%20-%20Loading%2C%20Saving%20and%20Freezing%20Embeddings.ipynb)\n",
    "\n",
    "我们知道，在NLP领域，预训练语言模型已经发挥着越来越大的作用，在本附录中，我们将介绍：如何加载自定义单词嵌入，如何在训练我们的模型时固定和解除word embedding，以及如何保存我们学到的embedding，以便它们可以在其他模型中使用。\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "* http://anie.me/On-Torchtext/\n",
    "* http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "* https://github.com/spro/practical-pytorch\n",
    "* https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n",
    "* https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "* https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py\n",
    "* https://github.com/Shawn1993/cnn-text-classification-pytorch\n",
    "* https://github.com/bentrevett/pytorch-sentiment-analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  CountVectorizer()\n",
    "import pandas as pd\n",
    "stopword = pd.read_csv('https://raw.githubusercontent.com/goto456/stopwords/master/baidu_stopwords.txt', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = []\n",
    "for i in stopword.values:\n",
    "    stoplist.append(str(i[0]))\n",
    "for i in ['风险' ,'不太']:\n",
    "    stoplist.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  CountVectorizer(stop_words = stoplist)\n",
    "import jieba\n",
    "import re\n",
    "def cut_word(sent):\n",
    "    line = re.sub(r'[a-zA-Z0-9]*' , '' ,sent)\n",
    "    wordlist = jieba.lcut(line , cut_all = False)\n",
    "    return \" \".join([word for word in wordlist if word not in stopword and len(word)>1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'否定 这个 风险'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_word(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = '这个风险不太大 ！'\n",
    "sentence2 = '否定了这个风险 ！'\n",
    "list1=np.array(  [sentence1 , sentence2])\n",
    "import numpy as np\n",
    "data = pd.DataFrame([sentence1 , sentence2] , index = np.arange(2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['transform'] = data.apply(lambda x:cut_word(str(x)) , axis = 1)\n",
    "\n",
    "# tokenizer.fit_transform(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>不太大</th>\n",
       "      <th>否定</th>\n",
       "      <th>这个</th>\n",
       "      <th>风险</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   不太大  否定  这个  风险\n",
       "0    1   0   1   1\n",
       "1    0   1   1   1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv =  CountVectorizer()\n",
    "data_cv = cv.fit_transform(data['transform'])\n",
    "data_dtm = pd.DataFrame(data_cv.toarray() , columns = cv.get_feature_names())\n",
    "data_dtm.index = data.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'不太'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\anaconda\\lib\\site-packages)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 173, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 203, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 315, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 472, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 341, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 140, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 128, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 29, in _iter_built\n",
      "    for version, func in infos:\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 272, in iter_index_candidate_infos\n",
      "    result = self._finder.find_best_candidate(\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\index\\package_finder.py\", line 851, in find_best_candidate\n",
      "    candidates = self.find_all_candidates(project_name)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\index\\package_finder.py\", line 798, in find_all_candidates\n",
      "    page_candidates = list(page_candidates_it)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\index\\sources.py\", line 134, in page_candidates\n",
      "    yield from self._candidates_from_page(self._link)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\index\\package_finder.py\", line 758, in process_project_url\n",
      "    html_page = self._link_collector.fetch_page(project_url)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\index\\collector.py\", line 490, in fetch_page\n",
      "    return _get_html_page(location, session=self.session)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\index\\collector.py\", line 400, in _get_html_page\n",
      "    resp = _get_html_response(url, session=session)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\index\\collector.py\", line 115, in _get_html_response\n",
      "    resp = session.get(\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\requests\\sessions.py\", line 555, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_internal\\network\\session.py\", line 454, in request\n",
      "    return super().request(method, url, *args, **kwargs)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\requests\\sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\requests\\sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\adapter.py\", line 53, in send\n",
      "    resp = super(CacheControlAdapter, self).send(request, **kw)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\requests\\adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\urllib3\\connectionpool.py\", line 696, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\urllib3\\connectionpool.py\", line 964, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\urllib3\\connection.py\", line 359, in connect\n",
      "    conn = self._connect_tls_proxy(hostname, conn)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\urllib3\\connection.py\", line 500, in _connect_tls_proxy\n",
      "    return ssl_wrap_socket(\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\urllib3\\util\\ssl_.py\", line 453, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "  File \"C:\\anaconda\\lib\\site-packages\\pip\\_vendor\\urllib3\\util\\ssl_.py\", line 495, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock)\n",
      "  File \"C:\\anaconda\\lib\\ssl.py\", line 500, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\anaconda\\lib\\ssl.py\", line 997, in _create\n",
      "    raise ValueError(\"check_hostname requires server_hostname\")\n",
      "ValueError: check_hostname requires server_hostname\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
